{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import pandas\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2-sat', 'binary search', 'bitmasks', 'brute force', 'chinese remainder theorem', 'combinatorics', 'constructive algorithms', 'data structures', 'dfs and similar', 'divide and conquer', 'dp', 'dsu', 'expression parsing', 'fft', 'flows', 'games', 'geometry', 'graph matchings', 'graphs', 'greedy', 'hashing', 'implementation', 'math', 'matrices', 'meet-in-the-middle', 'number theory', 'probabilities', 'schedules', 'shortest paths', 'sortings', 'special problem', 'string suffix structures', 'strings', 'ternary search', 'trees', 'two pointers']\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv('words_all_no_repeats.csv')\n",
    "allWords = df['main_text'].str.cat(sep=' ').split(' ')\n",
    "tags = set()\n",
    "for tag_list in df['tags']:\n",
    "    tag_list_parsed = ast.literal_eval(tag_list.replace('/', ','))\n",
    "    for tag in tag_list_parsed:\n",
    "        tags.add(tag)\n",
    "allTags = sorted(list(tags))\n",
    "pickle.dump(allTags, open('models/tags.pkl', 'wb'))\n",
    "print(allTags)\n",
    "# allTags = list(set(df['tags'].str.cat(sep=' ').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    # print(count)\n",
    "    dictionary = dict()\n",
    "    # Add words to dictionary based off of how common they are\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "vocabularyF, vocabularyR = build_dataset(allWords)\n",
    "#print(vocabularyF)\n",
    "vocab_size = len(vocabularyF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Training tag: 2-sat\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           664992    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 718,293\n",
      "Trainable params: 718,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "2112/3000 [====================>.........] - ETA: 10s - loss: 0.0700 - acc: 0.9830"
     ]
    }
   ],
   "source": [
    "text = df['main_text'].as_matrix()\n",
    "# Let's try binary classification\n",
    "models = []\n",
    "scores = []\n",
    "filename = 'models/model{}.h5'\n",
    "for i, goal_tag in enumerate(allTags):\n",
    "    if i < 17:\n",
    "        continue\n",
    "    print('{}. Training tag: {}'.format(i + 1, goal_tag))\n",
    "    # Gets indices for each word in line, for each line in main_text\n",
    "    data = [[vocabularyF.get(i, 0) for i in j.split(' ')] for j in text]\n",
    "    labels = [1 if i == goal_tag else 0 for i in df['tags'].as_matrix()]\n",
    "    train_data = data[:3000]\n",
    "    y_train = np.asarray(labels[:3000])\n",
    "    test_data = data[3000:]\n",
    "    y_test = np.asarray(labels[3000:])\n",
    "    max_review_length = 500\n",
    "    X_train = sequence.pad_sequences(train_data, maxlen=max_review_length)\n",
    "    X_test = sequence.pad_sequences(test_data, maxlen=max_review_length)\n",
    "\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=64)\n",
    "    # Final evaluation of the model\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (score[1]*100))\n",
    "    save_model(model, filename.format(i))\n",
    "    models.append(model)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "filename = 'models/model{}.h5'\n",
    "from keras.models import load_model\n",
    "for i, tag in enumerate(allTags):\n",
    "    print('loading {}'.format(tag))\n",
    "    models.append(load_model(filename.format(i)))\n",
    "\n",
    "#Let's test on trial 3005\n",
    "results = []\n",
    "for i, model in enumerate(models):\n",
    "    print(model.predict(data[3005]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
