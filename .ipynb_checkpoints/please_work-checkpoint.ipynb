{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import pandas\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('words_all.csv')\n",
    "allWords = df['main_text'].str.cat(sep=' ').split(' ')\n",
    "allTags = list(set(df['tag'].str.cat(sep=' ').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    # print(count)\n",
    "    dictionary = dict()\n",
    "    # Add words to dictionary based off of how common they are\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "vocabularyF, vocabularyR = build_dataset(allWords)\n",
    "#print(vocabularyF)\n",
    "vocab_size = len(vocabularyF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           590048    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 643,349\n",
      "Trainable params: 643,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alyssa\\anaconda2\\envs\\python3.5\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6403/6403 [==============================] - 88s 14ms/step - loss: 0.0238 - acc: 0.9938\n",
      "Epoch 2/3\n",
      "6403/6403 [==============================] - 94s 15ms/step - loss: 0.0011 - acc: 0.9989\n",
      "Epoch 3/3\n",
      "6403/6403 [==============================] - 91s 14ms/step - loss: 0.0011 - acc: 0.9989\n",
      "Accuracy: 99.81%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           590048    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 643,349\n",
      "Trainable params: 643,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "6403/6403 [==============================] - 85s 13ms/step - loss: 0.0455 - acc: 0.9724\n",
      "Epoch 2/3\n",
      "6403/6403 [==============================] - 91s 14ms/step - loss: 0.0225 - acc: 0.9775\n",
      "Epoch 3/3\n",
      "6403/6403 [==============================] - 94s 15ms/step - loss: 0.0224 - acc: 0.9775\n",
      "Accuracy: 97.81%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           590048    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 643,349\n",
      "Trainable params: 643,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "5824/6403 [==========================>...] - ETA: 8s - loss: 0.0301 - acc: 0.9887"
     ]
    }
   ],
   "source": [
    "text = df['main_text'].as_matrix()\n",
    "# Let's try binary classification\n",
    "models = []\n",
    "scores = []\n",
    "for goal_tag in allTags:\n",
    "    # Gets indices for each word in line, for each line in main_text\n",
    "    data = [[vocabularyF.get(i, 0) for i in j.split(' ')] for j in text]\n",
    "    labels = [1 if i == goal_tag else 0 for i in df['tag'].as_matrix()]\n",
    "    train_data = data[:6403]\n",
    "    y_train = np.asarray(labels[:6403])\n",
    "    test_data = data[6403:]\n",
    "    y_test = np.asarray(labels[6403:])\n",
    "    max_review_length = 500\n",
    "    X_train = sequence.pad_sequences(train_data, maxlen=max_review_length)\n",
    "    X_test = sequence.pad_sequences(test_data, maxlen=max_review_length)\n",
    "\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "    # Final evaluation of the model\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (score[1]*100))\n",
    "    models.append(model)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [models, scores]\n",
    "pickle.dump(results, open(\"results.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
