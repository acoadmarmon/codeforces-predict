{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import pandas\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2-sat', 'binary search', 'bitmasks', 'brute force', 'chinese remainder theorem', 'combinatorics', 'constructive algorithms', 'data structures', 'dfs and similar', 'divide and conquer', 'dp', 'dsu', 'expression parsing', 'fft', 'flows', 'games', 'geometry', 'graph matchings', 'graphs', 'greedy', 'hashing', 'implementation', 'math', 'matrices', 'meet-in-the-middle', 'number theory', 'probabilities', 'schedules', 'shortest paths', 'sortings', 'special problem', 'string suffix structures', 'strings', 'ternary search', 'trees', 'two pointers']\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv('words_all_no_repeats.csv')\n",
    "allWords = df['main_text'].str.cat(sep=' ').split(' ')\n",
    "tags = set()\n",
    "for tag_list in df['tags']:\n",
    "    tag_list_parsed = ast.literal_eval(tag_list.replace('/', ','))\n",
    "    for tag in tag_list_parsed:\n",
    "        tags.add(tag)\n",
    "allTags = sorted(list(tags))\n",
    "pickle.dump(allTags, open('models/tags.pkl', 'wb'))\n",
    "print(allTags)\n",
    "# allTags = list(set(df['tags'].str.cat(sep=' ').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    # print(count)\n",
    "    dictionary = dict()\n",
    "    # Add words to dictionary based off of how common they are\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "vocabularyF, vocabularyR = build_dataset(allWords)\n",
    "#print(vocabularyF)\n",
    "vocab_size = len(vocabularyF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Training tag: 2-sat\n",
      "Label exists\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 32)           664992    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 718,293\n",
      "Trainable params: 718,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 47s 16ms/step - loss: 0.0560 - acc: 0.9860\n",
      "Accuracy: 99.62%\n",
      "2. Training tag: binary search\n",
      "Label exists\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 500, 32)           664992    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 718,293\n",
      "Trainable params: 718,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "2560/3000 [========================>.....] - ETA: 6s - loss: 0.1182 - acc: 0.9086"
     ]
    }
   ],
   "source": [
    "text = df['main_text'].as_matrix()\n",
    "# Let's try binary classification\n",
    "models = []\n",
    "scores = []\n",
    "filename = 'models/model{}.h5'\n",
    "for i, goal_tag in enumerate(allTags):\n",
    "\n",
    "    print('{}. Training tag: {}'.format(i + 1, goal_tag))\n",
    "    # Gets indices for each word in line, for each line in main_text\n",
    "    data = [[vocabularyF.get(i, 0) for i in j.split(' ')] for j in text]\n",
    "    labels = [1 if goal_tag in j else 0 for j in df['tags'].as_matrix()]\n",
    "    if 1 in labels:\n",
    "        print('Label exists')\n",
    "    else:\n",
    "        print('no data')\n",
    "    train_data = data[:3000]\n",
    "    y_train = np.asarray(labels[:3000])\n",
    "    test_data = data[3000:]\n",
    "    y_test = np.asarray(labels[3000:])\n",
    "    max_review_length = 500\n",
    "    X_train = sequence.pad_sequences(train_data, maxlen=max_review_length)\n",
    "    X_test = sequence.pad_sequences(test_data, maxlen=max_review_length)\n",
    "\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=64)\n",
    "    # Final evaluation of the model\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (score[1]*100))\n",
    "    save_model(model, filename.format(i))\n",
    "    models.append(model)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 2-sat\n",
      "loading binary search\n",
      "loading bitmasks\n",
      "loading brute force\n",
      "loading chinese remainder theorem\n",
      "loading combinatorics\n",
      "loading constructive algorithms\n",
      "loading data structures\n",
      "loading dfs and similar\n",
      "loading divide and conquer\n",
      "loading dp\n",
      "loading dsu\n",
      "loading expression parsing\n",
      "loading fft\n",
      "loading flows\n",
      "loading games\n",
      "loading geometry\n",
      "loading graph matchings\n",
      "loading graphs\n",
      "loading greedy\n",
      "loading hashing\n",
      "loading implementation\n",
      "loading math\n",
      "loading matrices\n",
      "loading meet-in-the-middle\n",
      "loading number theory\n",
      "loading probabilities\n",
      "loading schedules\n",
      "loading shortest paths\n",
      "loading sortings\n",
      "loading special problem\n",
      "loading string suffix structures\n",
      "loading strings\n",
      "loading ternary search\n",
      "loading trees\n",
      "loading two pointers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected embedding_5_input to have shape (None, 500) but got array with shape (177, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-34a055715c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alyssa\\anaconda2\\envs\\test-py\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alyssa\\anaconda2\\envs\\test-py\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1770\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1771\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1772\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alyssa\\anaconda2\\envs\\test-py\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    151\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected embedding_5_input to have shape (None, 500) but got array with shape (177, 1)"
     ]
    }
   ],
   "source": [
    "#  Testing on individual test cases\n",
    "filename = 'models/model{}.h5'\n",
    "from keras.models import load_model\n",
    "if not models:\n",
    "    for i, tag in enumerate(allTags):\n",
    "        print('loading {}'.format(tag))\n",
    "        models.append(load_model(filename.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 2-sat?  no\t[ 0.00031093]\n",
      "Is binary search?  no\t[ 0.00021245]\n",
      "Is bitmasks?  no\t[  6.71432572e-05]\n",
      "Is brute force?  no\t[ 0.00027262]\n",
      "Is chinese remainder theorem?  no\t[  6.36333207e-05]\n",
      "Is combinatorics?  no\t[  7.59445829e-05]\n",
      "Is constructive algorithms?  no\t[ 0.00021703]\n",
      "Is data structures?  no\t[ 0.00039493]\n",
      "Is dfs and similar?  no\t[ 0.00038267]\n",
      "Is divide and conquer?  no\t[ 0.00036098]\n",
      "Is dp?  no\t[ 0.00013596]\n",
      "Is dsu?  no\t[ 0.0004459]\n",
      "Is expression parsing?  no\t[ 0.0008421]\n",
      "Is fft?  no\t[ 0.0005758]\n",
      "Is flows?  no\t[ 0.00026128]\n",
      "Is games?  no\t[ 0.00043417]\n",
      "Is geometry?  no\t[ 0.00015261]\n",
      "Is graph matchings?  no\t[ 0.00023076]\n",
      "Is graphs?  no\t[ 0.00055019]\n",
      "Is greedy?  no\t[ 0.00033246]\n",
      "Is hashing?  no\t[ 0.00052827]\n",
      "Is implementation?  no\t[ 0.00054067]\n",
      "Is math?  no\t[ 0.0001674]\n",
      "Is matrices?  no\t[  6.65249172e-05]\n",
      "Is meet-in-the-middle?  no\t[ 0.00022687]\n",
      "Is number theory?  no\t[ 0.00043508]\n",
      "Is probabilities?  no\t[ 0.00103406]\n",
      "Is schedules?  no\t[ 0.0003296]\n",
      "Is shortest paths?  no\t[ 0.00032429]\n",
      "Is sortings?  no\t[ 0.00019797]\n",
      "Is special problem?  no\t[ 0.00065948]\n",
      "Is string suffix structures?  no\t[ 0.00036178]\n",
      "Is strings?  no\t[ 0.00053968]\n",
      "Is ternary search?  no\t[ 0.0006045]\n",
      "Is trees?  no\t[ 0.0005865]\n",
      "Is two pointers?  no\t[ 0.00019704]\n",
      "['bitmasks'/ 'dp'/ 'greedy']\n"
     ]
    }
   ],
   "source": [
    "#Let's test on trial 3005\n",
    "test_num = 3100\n",
    "results = []\n",
    "text = df['main_text'].as_matrix()\n",
    "data = [[vocabularyF.get(i, 0) for i in j.split(' ')] for j in text]\n",
    "train_data = [data[test_num]]\n",
    "X_train = sequence.pad_sequences(train_data, maxlen=500)\n",
    "for i, model in enumerate(models):\n",
    "    result = model.predict(X_train)[0]\n",
    "    print('Is {}?  {}\\t{}'.format(allTags[i], 'yes' if result > 1 - result else 'no', result))\n",
    "print(df['tags'][test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
